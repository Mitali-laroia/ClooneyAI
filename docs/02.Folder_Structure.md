# Folder Structure

This document describes the project structure for Clooney AI, a website cloning tool built with LangGraph, LangChain, and Playwright.

## Project Overview

Clooney AI is designed to clone websites using AI-powered analysis and browser automation. The project leverages:
- **LangGraph** for workflow orchestration
- **LangChain** for LLM integration
- **Playwright** for browser automation
- **Sandboxing** for safe execution

## Directory Structure

```
src/
├── agents/          # LangGraph agents for different cloning tasks
├── workflows/       # LangGraph workflows and graphs
├── tools/           # Custom tools (scraping, parsing, etc.)
├── models/          # Pydantic data models
├── services/        # Business logic services
├── browser/         # Playwright browser automation
├── sandbox/         # Sandboxing for safe execution
├── config/          # Configuration management
├── utils/           # Utility functions
├── graph/           # Graph-related code
├── prompts/         # LLM prompts
└── main.py          # Application entry point
```

## Module Descriptions

### `agents/`
Contains LangGraph agents that perform specific tasks in the cloning workflow. Agents are autonomous components that can analyze, scrape, and process web content.

**Purpose:**
- Define specialized agents for different cloning tasks
- Implement agent logic and decision-making
- Handle agent state and communication

### `workflows/`
Houses LangGraph workflows (graphs) that orchestrate the entire cloning process. Workflows connect multiple agents and tools to achieve complex tasks.

**Purpose:**
- Define multi-step cloning workflows
- Orchestrate agent interactions
- Manage workflow state and transitions

### `tools/`
Custom tools that agents can use to perform specific actions such as scraping, parsing HTML, extracting assets, etc.

**Purpose:**
- Implement reusable tools for agents
- Provide utility functions for web scraping
- Handle content extraction and processing

### `models/`
Pydantic data models that define the structure of data used throughout the application.

**Purpose:**
- Define data schemas for websites, pages, assets
- Ensure type safety and validation
- Provide clear data contracts

### `services/`
Business logic services that handle core functionality like cloning coordination, storage management, and processing.

**Purpose:**
- Implement high-level business logic
- Coordinate between different modules
- Handle complex operations

### `browser/`
Browser automation layer using Playwright to interact with websites, capture screenshots, and extract content.

**Purpose:**
- Manage Playwright browser instances
- Handle page navigation and interaction
- Capture screenshots and network requests
- Extract DOM content and assets

### `sandbox/`
Sandboxing module for safely executing potentially untrusted code or operations.

**Purpose:**
- Provide isolated execution environment
- Enforce resource limits (CPU, memory, time)
- Prevent malicious code execution

### `config/`
Configuration management for application settings, environment variables, and user preferences.

**Purpose:**
- Define application settings
- Load environment variables
- Manage configuration profiles

### `utils/`
Utility functions and helper modules used across the application.

**Purpose:**
- Provide common utility functions
- Handle file operations
- Implement shared helpers

### `graph/`
Graph-related code for building and managing LangGraph structures.

**Purpose:**
- Define graph nodes and edges
- Implement graph execution logic
- Handle graph state management

### `prompts/`
LLM prompts used for AI-powered analysis and decision-making.

**Purpose:**
- Store prompt templates
- Manage prompt versioning
- Organize domain-specific prompts

## File Naming Conventions

- Use **snake_case** for Python files and directories
- Use descriptive names that indicate functionality
- Group related files in subdirectories when appropriate

## Extension Points

The modular structure allows for easy extension:
- Add new agents in `agents/` for specialized tasks
- Create new workflows in `workflows/` for different cloning strategies
- Implement custom tools in `tools/` for specific extraction needs
- Define new models in `models/` as requirements evolve
